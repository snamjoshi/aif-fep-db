{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization of author network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bibtexparser\n",
    "import logging\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import os\n",
    "os.chdir(\"..\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from src.db import Database\n",
    "from src.tags import Tags\n",
    "\n",
    "LOGGER = logging.getLogger(__name__)\n",
    "logging.basicConfig(level = logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pubmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_metadata(db, filter=True):\n",
    "    # Initialize Tag class\n",
    "    metadata = Tags()\n",
    "\n",
    "    # Load the tag file and turn into a dataframe\n",
    "    metadata.load(\"data/tag_files/tags.yaml\")\n",
    "    md = pd.DataFrame(metadata.tags[\"tagged_papers\"])\n",
    "    \n",
    "    # Merge with database\n",
    "    mddb = db.merge(md, on=\"id\", how=\"outer\")\n",
    "    \n",
    "    # List current progress out of total.\n",
    "    total_papers = mddb.shape[0]\n",
    "    tagged_papers = sum(~mddb[\"tag\"].isnull())\n",
    "    percent = np.round((tagged_papers/total_papers) * 100, 2)\n",
    "    \n",
    "    LOGGER.info(f\"Currently {tagged_papers}/{total_papers} ({percent}%) papers tagged.\")\n",
    "    LOGGER.info(f\"{total_papers-tagged_papers} papers remaining.\")\n",
    "        \n",
    "    mddb = mddb.reset_index()\n",
    "        \n",
    "    return mddb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.db:Checking tables...\n",
      "INFO:src.db:Loading tables...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.db:Tables downloaded from PubMed on Thursday, Sept. 14, 2023.\n",
      "INFO:src.tags:YAML tag file successfully loaded from data/tag_files/tags.yaml.\n",
      "INFO:__main__:Currently 3585/3585 (100.0%) papers tagged.\n",
      "INFO:__main__:0 papers remaining.\n"
     ]
    }
   ],
   "source": [
    "# Set tables\n",
    "tables = [\"active_inference\", \"bayesian_mechanics\", \"free_energy\", \"friston\", \"karl_friston\", \"predictive_coding\", \"predictive_processing\"]\n",
    "\n",
    "# Load database\n",
    "database = Database()\n",
    "database.load(tables=tables)\n",
    "db = database.db\n",
    "\n",
    "# Load metadata\n",
    "pubmed_db = load_metadata(db, filter=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load BioArXiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"examples/bioarxiv_aif_citations.bib\") as bibtex_file:\n",
    "    bib_database = bibtexparser.load(bibtex_file)\n",
    "    \n",
    "bioarxiv_db = pd.DataFrame(bib_database.entries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load ArXiv and PsyArXiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_psyarxiv_db = pd.read_csv(\"data/arxiv_papers.csv\")\n",
    "arxiv_db = arxiv_psyarxiv_db[arxiv_psyarxiv_db[\"Archive\"] == \"ArXiv\"]\n",
    "psyarxiv_db = arxiv_psyarxiv_db[arxiv_psyarxiv_db[\"Archive\"] == \"PsyArXiv\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isolate author list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pubmed_authors = pubmed_db[\"authors\"]\n",
    "bioarxiv_authors = bioarxiv_db[\"author\"]\n",
    "arxiv_authors = arxiv_db[\"Authors\"]\n",
    "psyarxiv_authors = psyarxiv_db[\"Authors\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize author list\n",
    "\n",
    "Authors are currently in the following format\n",
    "* Pubmed Authors: Last Name First Initial, Last Name First Initial, ...\n",
    "* BioArXiv Authors: Last Name, First Name and Last Name, First Name and ...\n",
    "* ArXiv Authors: [First Name Last Name, First Name Last Name, ...]\n",
    "* PsyArXiv Authors: [First Name Last Name, First Name Last Name, ...]\n",
    "\n",
    "Based on this, it seems that the Pubmed style will have to become the global style because I am missing initials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize BioArXiv list\n",
    "def clean_special_characters(name):\n",
    "    # Replace specific special character sequences with desired replacements\n",
    "    name = re.sub(r'\\{\\\\\\'a\\}', 'a', name)\n",
    "    name = re.sub(r'\\{\\\\~n\\}', '~n', name)\n",
    "    return name\n",
    "\n",
    "def convert_name_format(name):\n",
    "    # name = clean_special_characters(name)                           # Clean special characters in the name\n",
    "    last_name, first_names = name.split(', ')                       # Split the name into last name and the rest\n",
    "    initials = ''.join([name[0] for name in first_names.split()])   # Split the first names into individual names\n",
    "    return f\"{last_name} {initials}\"                                # Return the formatted name\n",
    "\n",
    "def convert_bioarxiv_names(row):\n",
    "    # Clean the malformed \" , and \" in the authors string\n",
    "    cleaned_authors_string = re.sub(r' , and ', ' and ', row)\n",
    "\n",
    "    # Split the authors string using \" and \" as a delimiter\n",
    "    parts = cleaned_authors_string.split(' and ')\n",
    "\n",
    "    # Merge parts that were split incorrectly\n",
    "    author_list = []\n",
    "    temp_author = \"\"\n",
    "    for part in parts:\n",
    "        if ',' in part:\n",
    "            if temp_author:\n",
    "                author_list.append(temp_author)\n",
    "                temp_author = \"\"\n",
    "            author_list.append(part)\n",
    "        else:\n",
    "            temp_author += \" and \" + part\n",
    "\n",
    "    # Add the last part if any\n",
    "    if temp_author:\n",
    "        author_list.append(temp_author.lstrip(\" and \"))\n",
    "    \n",
    "    formatted_authors = [convert_name_format(name) for name in author_list]\n",
    "    return \", \".join(formatted_authors)\n",
    "\n",
    "bioarxiv_authors = bioarxiv_authors.drop(labels=[169, 276]).apply(convert_bioarxiv_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize ArXiv lists\n",
    "\n",
    "def convert_name_format2(name):\n",
    "    parts = name.split()    # Split the name into parts\n",
    "    last_name = parts[-1]   # The last part is the last name, the rest are first and middle names\n",
    "    \n",
    "    # Get the initials of the first and middle names\n",
    "    first_middle_names = parts[:-1]   \n",
    "    initials = ''.join([n[0] for n in first_middle_names])\n",
    "    return f\"{last_name} {initials}\"\n",
    "\n",
    "def convert_arxiv_psyarxiv_names(row):\n",
    "    # Remove the square brackets and split the string by commas\n",
    "    author_list = row.strip(\"[]\").split(\", \")\n",
    "\n",
    "    # Convert all the names in the list\n",
    "    formatted_authors = [convert_name_format2(name) for name in author_list]\n",
    "\n",
    "    # Join the formatted names with a comma\n",
    "    return ', '.join(formatted_authors)\n",
    "\n",
    "arxiv_authors = arxiv_authors.apply(convert_arxiv_psyarxiv_names)\n",
    "psyarxiv_authors = psyarxiv_authors.apply(convert_arxiv_psyarxiv_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = pd.concat([                         # Combine author lists together\n",
    "    pubmed_authors, \n",
    "    bioarxiv_authors, \n",
    "    psyarxiv_authors, \n",
    "    arxiv_authors]).reset_index(drop=True)\n",
    "\n",
    "authors = authors.tolist()                    # Convert to list\n",
    "authors = [a.strip(\".\") for a in authors if isinstance(a, str)]   # Strip out any periods that may have been included\n",
    "authors = [a.split(\",\") for a in authors] # Separate into a list of lists\n",
    "authors = [[p.strip() for p in a] for a in authors]  # Remove leading/trailing white space around author name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 335m 34.0s to generate\n",
    "unique_elem = sorted(set(sum(authors, []))) # sorted list of unique elements\n",
    "n = len(unique_elem)\n",
    "author_adj_mat = np.zeros((n,n), dtype= int) # matrix (n,n)\n",
    "\n",
    "for i, a in enumerate(unique_elem):\n",
    "    for j, b in enumerate(unique_elem):\n",
    "        author_adj_mat[i][j] = sum([True for seq in authors if a in seq and b in seq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and load\n",
    "# np.save(\"data/author_adj_mat.npy\", author_adj_mat, allow_pickle=True)\n",
    "\n",
    "# with open(\"data/author_adj_mat.npy\", \"rb\") as f:\n",
    "#     author_adj_mat = np.load(f, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create adjaceny matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "from pyvis.network import Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.from_numpy_matrix(np.matrix(author_adj_mat), create_using=nx.DiGraph)\n",
    "# layout = nx.spring_layout(G)\n",
    "# nx.draw(G, layout)\n",
    "# nx.draw_networkx_edge_labels(G, pos=layout)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_462772/1881724624.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'500px'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'500px'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# populates the nodes and edges data structures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_nx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mnt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'nx.html'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ds_env_1/lib/python3.9/site-packages/pyvis/network.py\u001b[0m in \u001b[0;36mfrom_nx\u001b[0;34m(self, nx_graph, node_size_transf, edge_weight_transf, default_node_size, default_edge_weight, show_edge_weights, edge_scaling)\u001b[0m\n\u001b[1;32m    713\u001b[0m                     \u001b[0;31m# replace provided weight value and pass to 'value' or 'width'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m                     \u001b[0me\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwidth_type\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"weight\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 715\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_edge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misolates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnx_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ds_env_1/lib/python3.9/site-packages/pyvis/network.py\u001b[0m in \u001b[0;36madd_edge\u001b[0;34m(self, source, to, **options)\u001b[0m\n\u001b[1;32m    380\u001b[0m                 if (\n\u001b[1;32m    381\u001b[0m                         \u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdest\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mto\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mfrm\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m                         \u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mfrm\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mto\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m                 ):\n\u001b[1;32m    384\u001b[0m                     \u001b[0;31m# edge already exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nt = Network('500px', '500px')\n",
    "# populates the nodes and edges data structures\n",
    "nt.from_nx(G)\n",
    "nt.show('nx.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_adj_mat = pd.DataFrame(author_adj_mat)\n",
    "author_adj_mat.index = unique_elem\n",
    "author_adj_mat.columns = unique_elem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_adj_mat.to_csv(\"data/author_adj_mat.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Crowl, Sam and Coleman, Maeve Bella and Chaphiv, Andrew and Naegle, Kristen M.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bioarxiv_authors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Crowl, Sam', 'Coleman, Maeve Bella', 'Chaphiv, Andrew', 'Naegle, Kristen M.']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[author.strip() for author in bioarxiv_authors[0].split(\"and\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['Crowl, Sam', 'Coleman, Maeve Bella', 'Chaphiv, Andrew John', 'Naegle, Kristen M.']\n",
    "['Crowl S', 'Coleman MB', 'Chaphiv AJ', 'Naegle KM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Crowl S', 'Coleman MB', 'Chaphiv AJ', 'Naegle KM']\n"
     ]
    }
   ],
   "source": [
    "# Function to convert the names to the desired format\n",
    "def convert_name_format(name):\n",
    "    # Split the name into last name and the rest\n",
    "    last_name, first_names = name.split(', ')\n",
    "    # Split the first names into individual names\n",
    "    initials = ''.join([name[0] for name in first_names.split()])\n",
    "    # Return the formatted name\n",
    "    return f\"{last_name} {initials}\"\n",
    "\n",
    "# Convert all the names in the list\n",
    "test_authors = ['Crowl, Sam', 'Coleman, Maeve Bella', 'Chaphiv, Andrew John', 'Naegle, Kristen M.']\n",
    "formatted_authors = [convert_name_format(name) for name in test_authors]\n",
    "\n",
    "# Print the result\n",
    "print(formatted_authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_special_characters(name):\n",
    "    # Replace specific special character sequences with desired replacements\n",
    "    name = re.sub(r'\\{\\\\\\'a\\}', 'a', name)\n",
    "    name = re.sub(r'\\{\\\\~n\\}', '~n', name)\n",
    "    return name\n",
    "\n",
    "def convert_name_format(name):\n",
    "    name = clean_special_characters(name)                           # Clean special characters in the name\n",
    "    last_name, first_names = name.split(', ')                       # Split the name into last name and the rest\n",
    "    initials = ''.join([name[0] for name in first_names.split()])   # Split the first names into individual names\n",
    "    return f\"{last_name} {initials}\"                                # Return the formatted name\n",
    "\n",
    "def convert_bioarxiv_names(row):\n",
    "    stripped_row = [author.strip() for author in row.split(\"and\")]\n",
    "    return [convert_name_format(name) for name in stripped_row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Brandt I', 'Meyniel F', 'Ochoa D', 'aCarlo B', \"O'Neil Aa\", '~nGomez P', 'Garcia M~']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Input string of authors\n",
    "authors_string = \"Brandt, I and Meyniel, Florent and Ochoa, David and {\\\\'a}Carlo, Bob and O'Neil, Anne {\\\\'a}Marie and {\\\\~n}Gomez, Pedro and Garcia, Maria {\\\\~n}Luisa\"\n",
    "\n",
    "# Function to clean special characters in the name\n",
    "def clean_special_characters(name):\n",
    "    name = re.sub(r'\\{\\\\\\'a\\}', 'a', name)\n",
    "    name = re.sub(r'\\{\\\\~n\\}', '~n', name)\n",
    "    return name\n",
    "\n",
    "# Function to convert the names to the desired format\n",
    "def convert_name_format(name):\n",
    "    name = clean_special_characters(name)\n",
    "    last_name, first_names = name.split(', ')\n",
    "    initials = ''.join([n[0] for n in first_names.split()])\n",
    "    return f\"{last_name} {initials}\"\n",
    "\n",
    "# Split the authors string using \" and \" as a delimiter\n",
    "parts = authors_string.split(' and ')\n",
    "\n",
    "# Merge parts that were split incorrectly\n",
    "author_list = []\n",
    "temp_author = \"\"\n",
    "for part in parts:\n",
    "    if ',' in part:\n",
    "        if temp_author:\n",
    "            author_list.append(temp_author)\n",
    "            temp_author = \"\"\n",
    "        author_list.append(part)\n",
    "    else:\n",
    "        temp_author += \" and \" + part\n",
    "\n",
    "# Add the last part if any\n",
    "if temp_author:\n",
    "    author_list.append(temp_author.lstrip(\" and \"))\n",
    "\n",
    "# Convert all the names in the list\n",
    "formatted_authors = [convert_name_format(name) for name in author_list]\n",
    "\n",
    "# Print the result\n",
    "print(formatted_authors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print Exceptions, index, and row content\n",
    "for i, row in enumerate(bioarxiv_authors):\n",
    "    try:\n",
    "        convert_bioarxiv_names(row)\n",
    "    except Exception as e: \n",
    "        print('Error at index {}: {!r}'.format(i, row))\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.split(r'(?<=\\w), (?=[A-Z][a-z]+)', bioarxiv_authors[25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sedlak B, Pujol VC, Donta PK, Dustdar S\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Input string of authors\n",
    "authors_string = \"[Boris Sedlak, Victor Casamayor Pujol, Praveen Kumar Donta, Schahram Dustdar]\"\n",
    "\n",
    "# Function to clean special characters in the name (though not required in this case)\n",
    "def clean_special_characters(name):\n",
    "    name = re.sub(r'\\{\\\\\\'a\\}', 'a', name)\n",
    "    name = re.sub(r'\\{\\\\~n\\}', '~n', name)\n",
    "    return name\n",
    "\n",
    "# Function to convert the names to the desired format\n",
    "def convert_name_format(name):\n",
    "    # Clean special characters in the name\n",
    "    name = clean_special_characters(name)\n",
    "    # Split the name into parts\n",
    "    parts = name.split()\n",
    "    # The last part is the last name, the rest are first and middle names\n",
    "    last_name = parts[-1]\n",
    "    first_middle_names = parts[:-1]\n",
    "    # Get the initials of the first and middle names\n",
    "    initials = ''.join([n[0] for n in first_middle_names])\n",
    "    # Return the formatted name\n",
    "    return f\"{last_name} {initials}\"\n",
    "\n",
    "# Remove the square brackets and split the string by commas\n",
    "author_list = authors_string.strip(\"[]\").split(\", \")\n",
    "\n",
    "# Convert all the names in the list\n",
    "formatted_authors = [convert_name_format(name) for name in author_list]\n",
    "\n",
    "# Join the formatted names with a comma\n",
    "result = ', '.join(formatted_authors)\n",
    "\n",
    "# Print the result\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_env_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
